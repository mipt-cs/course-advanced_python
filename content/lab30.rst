Нейросети и глубинное обучение (Deep Learning)
###############################################

:date: 2022-04-25 09:00
:summary: Классические методы машинного обучения, ч.2, модуль scikit-learn


.. default-role:: code

.. contents:: Содержание

.. role:: python(code)
   :language: python


* Использован материал из сайта machinelearning.ru

Полезные слайды_

.. _слайды: {static}/extra/lab30/Voron-ML-ANN-slides.pdf

Введение
=========

**Иску́сственные нейро́нная се́ть (artificial neural network, ANN)**, или просто нейронная сеть — это математическая модель, а также ее программные или аппаратные реализации, построенная в некотором смысле по образу и подобию сетей нервных клеток живого организма.

Нейронные сети — один из наиболее известных и старых методов машинного обучения. 

Идея метода сформировалась в процессе изучения работы мозга живых существ. Но нужно помнить, что ИНС гораздо проще своих прототипов, биологических нейронных сетей, до конца не изученных до сих пор. 

Основные моменты истории
++++++++++++++++++++++++++

1. В 1958 году Розенблаттом изобретен перцептрон. Перцептрон обретает популярность — его используют для распознавания образов, прогнозирования погоды и т. д. Казалось, что построение полноценного искусственного интеллекта уже не за горами. 

2. В 1969 году Марвин Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он неспособен решать некоторые задачи, связанные с инвариантностью представлений. Интерес к нейронным сетям резко спадает. 

3. 1974 год — Пол Дж. Вербос, и А. И. Галушкин одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов. Изобретение не привлекло особого внимания. 

4. 1982 год — после длительного упадка, интерес к нейросетям вновь возрастает. Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (так называемая сеть Хопфилда). Кохоненом представлены модели сети, обучающейся без учителя (Нейронная **сеть Кохонена (Kohonen maps)**), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.

5. 1986 год — Дэвидом И. Румельхартом, Дж. Е. Хинтоном и Рональдом Дж. Вильямсом и независимо и одновременно С. И. Барцевым и В. А. Охониным (Красноярская группа) переоткрыт и существенно развит метод обратного распространения ошибки. Начался взрыв интереса к обучаемым нейронным сетям. 

6. 2012 - Так называемая deep learning revolution. В 2012 году команда под руководством Джорджа Э. Даля выиграла конкурс «Merck Molecular Activity Challenge», используя многозадачные глубокие нейронные сети для прогнозирования биомолекулярной мишени одного препарата. В 2014 году группа Хохрейтера использовала глубокое обучение для выявления нецелевых и токсических эффектов химических веществ, присутствующих в окружающей среде, в питательных веществах, продуктах домашнего обихода и лекарствах, и выиграла «Tox21 Data Challenge» от Национального института здравоохранения США, Управления по санитарному надзору за качеством пищевых продуктов и медикаментов и NCATS.

7. Текущие проблемы: Ничего не известно о границах применимости моделей глубоких нейронных сетей. Не разгадан секрет их суперэффективности в одних задачах и неэффективности в других. Это задачи для грядущих поколений. 

Основная сущность
++++++++++++++++++++

Как и линейные методы классификации и регрессии, по сути нейронные сети выдают ответ вида: 

.. math::
    
    y(x, w) =f\bigl( \sum_{j = 1}^N w_j \phi_j(x)\bigr) , 

где f — нелинейная функция активации, :math:`w` — вектор весов, :math:`\phi` — нелинейные базисные функции. Обучение нейронных сетей состоит в выборе базисных функций и настройке весов.

Методы обучения
+++++++++++++++++

Градиентные методы - это широкий класс оптимизационных алгоритмов, используемых не только в машинном обучении. Здесь градиентный подход будет рассмотрен в качестве способа подбора вектора синаптических весов w в линейном классификаторе. Пусть 

.. math::
    
    y^*: \: X \to Y 

- целевая зависимость, известная только на объектах обучающей выборки: 

.. math::
    
    X^l \, = \, (x_i,y_i)_{i=1}^l, \; y_i \, = \, y^*(x_i).

Найдём алгоритм a(x, w), аппроксимирующий зависимость y^*. В случае линейного классификатора искомый алгоритм имеет вид:

.. math::

    a(x, w) = \varphi(\sum_{j=1}^n w_j x^j \, - \, w_0), 

где :math:`\varphi(z)` играет роль функции активации (в простейшем случае можно положить :math:`\varphi(z) \, = \, sign(z))`.

Согласно принципу минимизации эмпирического риска для этого достаточно решить оптимизационную задачу: :math:`Q(w) \, = \, \sum_{i=1}^l L(a(x_i, w), \, y_i) \to \min_w`, где L(a,y) - заданная функция потерь.

Для минимизации применим метод градиентного спуска (gradient descent). Это пошаговый алгоритм, на каждой итерации которого вектор w изменяется в направлении наибольшего убывания функционала Q (то есть в направлении антиградиента):

.. math::

    w \, {:=} \, w \, - \, \eta \nabla Q(w), 

где :math:`\eta` - положительный параметр, называемый темпом обучения (learning rate).

Возможно 2 основных подхода к реализации градиентного спуска:

+ Пакетный (batch), когда на каждой итерации обучающая выборка просматривается целиком, и только после этого изменяется w. Это требует больших вычислительных затрат.
+ Стохастический (stochastic/online), когда на каждой итерации алгоритма из обучающей выборки каким-то (случайным) образом выбирается только один объект. Таким образом вектор w настраивается на каждый вновь выбираемый объект. 

чаще всего используется именно стохастический градиентный спуск с мультистартом (из различных начальных точек), поскольку функционал качества нейросетевых алгоритмов как правило устроен очень сложно и имеет огромное число локальных минимумов разного масштаба. 


Виды нейросетей
================

Архитектуры и виды задач
+++++++++++++++++++++++++

Очень сжатый и качественный обзор_

.. _обзор: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/

.. image:: {static}/extra/lab30/DL-arch.png
   :width: 100%

.. image:: {static}/extra/lab30/MedLearn.png
   :width: 100%

* картинка с https://towardsai.net/p/l/applications-of-deep-learning-in-health-informatics

.. image:: {static}/extra/lab30/Mendeleev.png
   :width: 100%

* картинка с https://sparkd.ai/the-periodic-table-of-deep-learning

Библиотеки
++++++++++++

Одно из наиболее информативных сравнений_

.. _сравнений: https://www.linkedin.com/pulse/tensorflow-vs-keras-pytorch-theano-harpreet-singh-sachdev

* Theano_

.. _Theano: https://theano-pymc.readthedocs.io/en/latest/

Разработана монреальским университетом в 2007 году. Написана специально для символьных вычислений с функциями многократной вложенности.

+ : прозрачные механизмы распараллеливания, возможность генерации кода на C, тесная интеграция с numpy

* Tensorflow_

.. _Tensorflow: https://www.tensorflow.org/learn

Написан Google на C++ . Единственная библиотека, использующая графы как основной объект. (точнее, тензорные графы, tensor networks). Остальные являются по сути библиотеками для **символьных вычислений**. 

+ : лёгкая масштабируемость, функциональность, идеально подходит для многомерных данных

* Pytorch_

.. _Pytorch: https://pytorch.org/

Изначально написан на C как библиотека для LUA (привет любителям Lineage). Включает широкие возможности распараллеливания на GPU и CPU. Основная структура **Dynamic Computational Graph**

+ : гибкость, быстрая обучаемость моделей, широкие  возможности отладки (debugging) 

* keras_

.. _keras: https://keras.io/

Обычно используется как надстройка над theano, tensorflow. Написана на python. 

+ : быстрое , устойчивое и простое прототипирование

Пример СNN 
===========

CNN (Convolutional Neural Network, Свёрточные нейросети) - специальная архитектура искусственных нейронных сетей, предложенная Яном Лекуном в 1988 году, исспользуется в основном для обработки визуальной информации (изображений). 

конспект_

.. _конспект: {static}/extra/lab30/CNN.pdf

Типичная архитектура

.. image:: {static}/extra/lab30/cnn.jpeg
   :width: 100%


notebook_

.. _notebook: {static}/extra/lab30/NNArcitecture.ipynb